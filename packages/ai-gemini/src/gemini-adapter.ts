import { GoogleGenerativeAI, Content } from "@google/generative-ai";
import {
  BaseAdapter,
  convertLegacyStream,
  type AIAdapterConfig,
  type ChatCompletionOptions,
  type ChatCompletionResult,
  type ChatCompletionChunk,
  type TextGenerationOptions,
  type TextGenerationResult,
  type SummarizationOptions,
  type SummarizationResult,
  type EmbeddingOptions,
  type EmbeddingResult,
  type Message,
  type StreamChunk,
} from "@tanstack/ai";

export interface GeminiAdapterConfig extends AIAdapterConfig {
  apiKey: string;
}

const GEMINI_MODELS = [
  "gemini-pro",
  "gemini-pro-vision",
  "gemini-ultra",
  "gemini-1.5-pro",
  "gemini-1.5-flash",
  "embedding-001",
] as const;

const GEMINI_IMAGE_MODELS = [] as const;

export type GeminiModel = (typeof GEMINI_MODELS)[number];

export class GeminiAdapter extends BaseAdapter<
  typeof GEMINI_MODELS,
  typeof GEMINI_IMAGE_MODELS
> {
  name = "gemini";
  models = GEMINI_MODELS;
  imageModels = GEMINI_IMAGE_MODELS;
  private client: GoogleGenerativeAI;

  constructor(config: GeminiAdapterConfig) {
    super(config);
    this.client = new GoogleGenerativeAI(config.apiKey);
  }

  async chatCompletion(
    options: ChatCompletionOptions
  ): Promise<ChatCompletionResult> {
    const model = this.client.getGenerativeModel({
      model: options.model || "gemini-pro",
      generationConfig: {
        temperature: options.temperature,
        topP: options.topP,
        maxOutputTokens: options.maxTokens,
        stopSequences: options.stopSequences,
      },
    });

    const history = this.formatMessagesForGemini(options.messages.slice(0, -1));
    const lastMessage = options.messages[options.messages.length - 1];

    const chat = model.startChat({ history });
    const result = await chat.sendMessage(lastMessage.content || "");
    const response = await result.response;
    const text = response.text();

    // Estimate token usage
    const promptTokens = this.estimateTokens(
      options.messages.map((m) => m.content).join(" ")
    );
    const completionTokens = this.estimateTokens(text);

    return {
      id: this.generateId(),
      model: options.model || "gemini-pro",
      content: text,
      role: "assistant",
      finishReason: (response.candidates?.[0]?.finishReason as any) || "stop",
      usage: {
        promptTokens,
        completionTokens,
        totalTokens: promptTokens + completionTokens,
      },
    };
  }

  async *chatCompletionStream(
    options: ChatCompletionOptions
  ): AsyncIterable<ChatCompletionChunk> {
    const model = this.client.getGenerativeModel({
      model: options.model || "gemini-pro",
      generationConfig: {
        temperature: options.temperature,
        topP: options.topP,
        maxOutputTokens: options.maxTokens,
        stopSequences: options.stopSequences,
      },
    });

    const history = this.formatMessagesForGemini(options.messages.slice(0, -1));
    const lastMessage = options.messages[options.messages.length - 1];

    const chat = model.startChat({ history });
    const result = await chat.sendMessageStream(lastMessage.content || "");

    for await (const chunk of result.stream) {
      const text = chunk.text();
      if (text) {
        yield {
          id: this.generateId(),
          model: options.model || "gemini-pro",
          content: text,
          finishReason: (chunk.candidates?.[0]?.finishReason as any) || null,
        };
      }
    }
  }

  async *chatStream(
    options: ChatCompletionOptions
  ): AsyncIterable<StreamChunk> {
    // Use legacy stream converter for now
    // TODO: Implement native structured streaming for Gemini
    yield* convertLegacyStream(
      this.chatCompletionStream(options),
      options.model || "gemini-pro"
    );
  }

  async generateText(
    options: TextGenerationOptions
  ): Promise<TextGenerationResult> {
    const model = this.client.getGenerativeModel({
      model: options.model || "gemini-pro",
      generationConfig: {
        temperature: options.temperature,
        topP: options.topP,
        maxOutputTokens: options.maxTokens,
        stopSequences: options.stopSequences,
      },
    });

    const result = await model.generateContent(options.prompt);
    const response = await result.response;
    const text = response.text();

    const promptTokens = this.estimateTokens(options.prompt);
    const completionTokens = this.estimateTokens(text);

    return {
      id: this.generateId(),
      model: options.model || "gemini-pro",
      text,
      finishReason: (response.candidates?.[0]?.finishReason as any) || "stop",
      usage: {
        promptTokens,
        completionTokens,
        totalTokens: promptTokens + completionTokens,
      },
    };
  }

  async *generateTextStream(
    options: TextGenerationOptions
  ): AsyncIterable<string> {
    const model = this.client.getGenerativeModel({
      model: options.model || "gemini-pro",
      generationConfig: {
        temperature: options.temperature,
        topP: options.topP,
        maxOutputTokens: options.maxTokens,
        stopSequences: options.stopSequences,
      },
    });

    const result = await model.generateContentStream(options.prompt);

    for await (const chunk of result.stream) {
      const text = chunk.text();
      if (text) {
        yield text;
      }
    }
  }

  async summarize(options: SummarizationOptions): Promise<SummarizationResult> {
    const prompt = this.buildSummarizationPrompt(options, options.text);

    const model = this.client.getGenerativeModel({
      model: options.model || "gemini-pro",
      generationConfig: {
        temperature: 0.3,
        maxOutputTokens: options.maxLength || 500,
      },
    });

    const result = await model.generateContent(prompt);
    const response = await result.response;
    const summary = response.text();

    const promptTokens = this.estimateTokens(prompt);
    const completionTokens = this.estimateTokens(summary);

    return {
      id: this.generateId(),
      model: options.model || "gemini-pro",
      summary,
      usage: {
        promptTokens,
        completionTokens,
        totalTokens: promptTokens + completionTokens,
      },
    };
  }

  async createEmbeddings(options: EmbeddingOptions): Promise<EmbeddingResult> {
    const inputs = Array.isArray(options.input)
      ? options.input
      : [options.input];
    const embeddings: number[][] = [];

    const model = this.client.getGenerativeModel({
      model: options.model || "embedding-001",
    });

    for (const input of inputs) {
      const result = await model.embedContent(input);
      embeddings.push(result.embedding.values);
    }

    const promptTokens = inputs.reduce(
      (sum, input) => sum + this.estimateTokens(input),
      0
    );

    return {
      id: this.generateId(),
      model: options.model || "embedding-001",
      embeddings,
      usage: {
        promptTokens,
        totalTokens: promptTokens,
      },
    };
  }

  private formatMessagesForGemini(messages: Message[]): Content[] {
    return messages.map((msg) => ({
      role: msg.role === "assistant" ? "model" : "user",
      parts: [{ text: msg.content || "" }],
    }));
  }

  private buildSummarizationPrompt(
    options: SummarizationOptions,
    text: string
  ): string {
    let prompt = "You are a professional summarizer. ";

    switch (options.style) {
      case "bullet-points":
        prompt += "Provide a summary in bullet point format. ";
        break;
      case "paragraph":
        prompt += "Provide a summary in paragraph format. ";
        break;
      case "concise":
        prompt += "Provide a very concise summary in 1-2 sentences. ";
        break;
      default:
        prompt += "Provide a clear and concise summary. ";
    }

    if (options.focus && options.focus.length > 0) {
      prompt += `Focus on the following aspects: ${options.focus.join(", ")}. `;
    }

    prompt += `\n\nText to summarize:\n${text}\n\nSummary:`;

    return prompt;
  }

  private estimateTokens(text: string): number {
    // Rough approximation: 1 token â‰ˆ 4 characters
    return Math.ceil(text.length / 4);
  }
}
